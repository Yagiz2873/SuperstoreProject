# SuperstoreProjectFile-2
This file contains SQL queries for our project. In this stage, I'd like to Arda Kantık to help me to write those queries. We've found that Superstore hasn't got any null values for each column. Then we've removed one duplicated row in the dataset and copied it into new table. This was our dataset based on OrderId. We've checked our columns types by using INFORMATION_SCHEMA.COLUMNS.

In this dataset, the columns called sales,quantity and profit were the most important columns for us so, we've handled outliers for these columns. We've tried to remove outliers first for these columns but we've realized that we've removed %30 of dataset :). This was too much percantage to get rid of outliers directly, so we've capped our outliers instead. Thanks for Sine Gökhan's outlier analysis in Python, we've found that %5 - %95 for lower and upper limit was the best limits for capping process. Then we've performed capping process in the SQL.

Then we've performed data enriching. We've added new columns. ThisOrdersDayDiff shows day difference between OrderDate and ShipmentDate for a single order. OrderYear shows OrderDate's year. Revenue shows the net income (Revenue = Sales- ((Sales*Discount)/100)). 

Then we've started to RFM analysis. First we've grouped the dataset by customerid then we've found basketsize, recency, tenure, tenureinmonths, frequency, monetary, recency-frequency-monetary scores separately and RF - RFM scores for these customers. We've calculated some values as average and total separately to make our analyses better. Then we've found the customers' segments based on their recency and frequency scores. 

After these, we've started to look for insights by joining our tables (SuperstoreRFM-based on unique CustomerID, UpdatedNewSuperstore-based on unique OrderID). With the help of our rule based Churn Analysis in Python before, we've found that the segments called At Risk, Hibernating and Can not Lose have higher churn rates and revenues simultaneously so we've decided to campaign for the customers at the At Risk segment. We've ignored Hibernating and Can not lose segments because of lower recency / frequency scores compared to At Risk for this campaign. 

Superstore dataset generally shows online orders for the supermarket in USA. We've found that New York, Los Angeles, San Francisco, Philadelphia and Seattle had the most monetary from 2014 to 2017. The west and east of the country had the most monetary and profit. Then we've reviewed the sales based on their months and grouped these months by their seasons. We've realized that the supermarket's gotten the most monetary and profit in the autumn compared to other seasons in these four years so we've decided to perform our Market Basket Analysis by using Appriori Algorithm based on each seasonal orders and subcategories. (Basically we've separated our dataset based on seasons and use Appriori Algorithm based on subcategory column for the dataset to find subcategories' relations, antecedent-consequent-support values,confidence,lift values etc). We've found that Storage had the best relationship with papers and binders. (It makes sense, because people place papers in binders and then place binders in storage items in the office). So we've decided to campaign for these three. (My friend, İrfan Mızrakcı, can talk about the Appriori Algorithm in detail.) 

Then we've found the best selling products for the quantity and sales columns. 
